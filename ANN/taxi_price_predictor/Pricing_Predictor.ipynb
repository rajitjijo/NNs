{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "451fb7f9-2626-4362-aa92-03354a1d9f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23349da7-9c61-4e96-b786-4a3aca1ea563",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"NYCTaxiFares.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3844902-1fcc-4564-8909-9548a299508a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    120000.000000\n",
       "mean         10.040326\n",
       "std           7.500134\n",
       "min           2.500000\n",
       "25%           5.700000\n",
       "50%           7.700000\n",
       "75%          11.300000\n",
       "max          49.900000\n",
       "Name: fare_amount, dtype: float64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"fare_amount\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22234fb3-d399-4b1a-8310-fd8a2180a369",
   "metadata": {},
   "outputs": [],
   "source": [
    "def haversine_distance(df, lat1, long1, lat2, long2):\n",
    "    \"\"\"\n",
    "    Calculates the haversine distance between 2 sets of GPS coordinates in df\n",
    "    \"\"\"\n",
    "    r = 6371  # average radius of Earth in kilometers\n",
    "       \n",
    "    phi1 = np.radians(df[lat1])\n",
    "    phi2 = np.radians(df[lat2])\n",
    "    \n",
    "    delta_phi = np.radians(df[lat2]-df[lat1])\n",
    "    delta_lambda = np.radians(df[long2]-df[long1])\n",
    "     \n",
    "    a = np.sin(delta_phi/2)**2 + np.cos(phi1) * np.cos(phi2) * np.sin(delta_lambda/2)**2\n",
    "    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1-a))\n",
    "    d = (r * c) # in kilometers\n",
    "\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c5a212a-f00b-426f-859e-be84562ee672",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"dist_km\"] = haversine_distance(df, 'pickup_latitude', 'pickup_longitude', 'dropoff_latitude', 'dropoff_longitude')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f56cd8e-5267-4de0-89d8-da0be82f51bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 120000 entries, 0 to 119999\n",
      "Data columns (total 9 columns):\n",
      " #   Column             Non-Null Count   Dtype  \n",
      "---  ------             --------------   -----  \n",
      " 0   pickup_datetime    120000 non-null  object \n",
      " 1   fare_amount        120000 non-null  float64\n",
      " 2   fare_class         120000 non-null  int64  \n",
      " 3   pickup_longitude   120000 non-null  float64\n",
      " 4   pickup_latitude    120000 non-null  float64\n",
      " 5   dropoff_longitude  120000 non-null  float64\n",
      " 6   dropoff_latitude   120000 non-null  float64\n",
      " 7   passenger_count    120000 non-null  int64  \n",
      " 8   dist_km            120000 non-null  float64\n",
      "dtypes: float64(6), int64(2), object(1)\n",
      "memory usage: 8.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc5a66e8-f223-4992-a8b0-32ed39e0a59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"pickup_datetime\"] = pd.to_datetime(df[\"pickup_datetime\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dad14adc-38e1-4c99-81ee-8e331d8e28b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"EDTdate\"] = df[\"pickup_datetime\"] - pd.Timedelta(hours=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9bd2be19-ba6e-45fb-9795-625d9e5b352f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"hour\"] = df[\"EDTdate\"].dt.hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7b93a17c-6b5f-4d3e-aa9c-554caa29ea9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"AMorPM\"] = np.where(df[\"hour\"]<12,\"am\", \"pm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f268379c-607c-40ef-8be6-30bdda71e032",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"day_week\"] =df[\"EDTdate\"].dt.strftime(\"%a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "81973a08-617d-4483-b89f-8367ca816724",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['pickup_datetime', 'fare_amount', 'fare_class', 'pickup_longitude',\n",
       "       'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude',\n",
       "       'passenger_count', 'dist_km', 'EDTdate', 'hour', 'AMorPM', 'day_week'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "79303e7a-2855-4c20-88ab-cd490c0e1791",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = [\"hour\", \"AMorPM\", \"day_week\"]\n",
    "cont_cols = ['pickup_longitude','pickup_latitude', 'dropoff_longitude', 'dropoff_latitude', 'passenger_count', 'dist_km']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4b0605c6-c4a5-474c-a0c6-8670d766354b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_col = [\"fare_amount\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b6629f8d-25be-4beb-b514-620516235ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cat in cat_cols:\n",
    "    df[cat] = df[cat].astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9556107b-5451-4d08-a09f-0489ac07e45f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['am', 'pm'], dtype='object')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"AMorPM\"].cat.categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "45a65e58-2024-436a-8544-6f7b5004c279",
   "metadata": {},
   "outputs": [],
   "source": [
    "hour = df[\"hour\"].cat.codes.values\n",
    "ampm = df[\"AMorPM\"].cat.codes.values\n",
    "day = df[\"day_week\"].cat.codes.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c07edc1e-3f85-4b29-a2e4-3b5cda96eb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cats = torch.tensor(np.stack([df[cat].cat.codes.values for cat in cat_cols],1), dtype=torch.long) #the clean way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b1aea5eb-edd9-44c8-b48b-4213b4fca948",
   "metadata": {},
   "outputs": [],
   "source": [
    "cont = torch.tensor(np.stack([df[cont].values for cont in cont_cols],1), dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "17aa9fbe-dcc4-4058-adb4-a04b3a90e303",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.tensor(df[y_col].values, dtype = torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "05109013-52d3-4eb5-80c8-c41f9911dae7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([120000, 3])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cats.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2b5473a2-5b3d-45e4-9b03-774daffb4942",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_szs = [len(df[cat].cat.categories) for cat in cat_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2e9d5392-f141-486c-817a-428dbd921f9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[24, 2, 7]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_szs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45f68e9-8f02-4ce9-b28f-f5727cd3b36a",
   "metadata": {},
   "source": [
    "The following line is needed to calculate the type of embeddings needed for categorical data <br>\n",
    "1- The size will be the number of categories in that categorical column  \n",
    "2- The dimension will the shape of the embedding vector that would represent one categorical datapoint\n",
    "    IT is typically best to keep the dimension between 50 and half of the number of categories, whichever is lesser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0c7e4cec-c779-46c1-a23b-df964dc864ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_szs = [(size, min(50, (size+1)//2)) for size in cat_szs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7d95a271-4314-42a2-84e9-bde7ec836e22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(24, 12), (2, 1), (7, 4)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_szs #so basically now for the hour categorical column each data point would get a 12D vecctor to be its representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "029a8430-6729-46d7-84a2-09c6684b454d",
   "metadata": {},
   "outputs": [],
   "source": [
    "selfembds = nn.ModuleList([nn.Embedding(ne,ed) for ne,ed in emb_szs]) #Creating an embedding layer using pytorch that would generate the emb dim sized vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0ae97b1a-fc6d-4233-997c-891f76585e6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0): Embedding(24, 12)\n",
       "  (1): Embedding(2, 1)\n",
       "  (2): Embedding(7, 4)\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selfembds #using module list we have made it into a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cd78c0c9-a2be-48f9-91e5-a35bc3276323",
   "metadata": {},
   "outputs": [],
   "source": [
    "catz = cats[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf95a6f-1576-4af2-8992-f9735e941dbb",
   "metadata": {},
   "source": [
    "So now we are passing each column of our categorical data to its appropriate embedding layer <br>\n",
    "\n",
    "0th embedding layer will pass all the values from the hour category column <br>\n",
    "1st embedding layer will pass all the values from the ampm category column, so on and so forth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "39ac0466-e984-42c4-be18-be5cbe84f295",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = []\n",
    "for i, e in enumerate(selfembds):\n",
    "    embeddings.append(e(catz[:,i])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "97d9adcd-1d12-40ba-88f2-f3acc69ac536",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddingz = torch.cat(embeddings, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "51682080-20ef-44f3-bb25-10cde077f7ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([120000, 6])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cont.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "40df56f9-5b50-4074-bd01-ae1ef37f2358",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabularModel(nn.Module):\n",
    "    \"\"\"\n",
    "    emb_szs = (no_of_categories, embedding dimension) to convert categorical to embeddings\n",
    "    n_cont = number of continuous features\n",
    "    out_sz = our desired output dimension\n",
    "    layers = list of number of neurons per layer from 2nd to l-1\n",
    "            ex : [100,200,300] \n",
    "            100 is the output of the fcn block and the input for the second fcn block\n",
    "            200 is the input of the second fcn block and input for the third, so on and so forth\n",
    "            This allows dynamic construction of the architecture based on user input\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, emb_szs, n_cont, out_sz, layers, p=0.5):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embeds = nn.ModuleList([nn.Embedding(ne,ed) for ne,ed in emb_szs])\n",
    "        self.embdrop = nn.Dropout(p)\n",
    "        self.normcont = nn.BatchNorm1d(n_cont)\n",
    "\n",
    "        layerlist = []\n",
    "\n",
    "        n_cat = sum([nf for ne, nf in emb_szs])\n",
    "        n_in = n_cat + n_cont\n",
    "\n",
    "        for i in layers:\n",
    "            layerlist.append(nn.Linear(n_in, i))\n",
    "            layerlist.append(nn.ReLU(inplace=True))\n",
    "            layerlist.append(nn.BatchNorm1d(i))\n",
    "            layerlist.append(nn.Dropout(p))\n",
    "            n_in = i\n",
    "\n",
    "        layerlist.append(nn.Linear(layers[-1], out_sz))\n",
    "        self.layers = nn.Sequential(*layerlist)\n",
    "\n",
    "    def forward(self, x_cat, x_cont):\n",
    "\n",
    "        embeddings = []\n",
    "        \n",
    "        for i,e in enumerate(self.embeds):\n",
    "            embeddings.append(e(x_cat[:,i]))\n",
    "\n",
    "        x = torch.cat(embeddings,1)\n",
    "        x = self.embdrop(x)\n",
    "\n",
    "        x_cont = self.normcont(x_cont)\n",
    "        x = torch.cat([x, x_cont], 1)\n",
    "\n",
    "        x = self.layers(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1126f085-a1a8-428e-b97c-c3fcf6e2c5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(33)\n",
    "model = TabularModel(emb_szs, cont.shape[1], 1, [200,100], 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8888cf41-53d9-48da-895a-5a8cbd85e1ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TabularModel(\n",
       "  (embeds): ModuleList(\n",
       "    (0): Embedding(24, 12)\n",
       "    (1): Embedding(2, 1)\n",
       "    (2): Embedding(7, 4)\n",
       "  )\n",
       "  (embdrop): Dropout(p=0.5, inplace=False)\n",
       "  (normcont): BatchNorm1d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (layers): Sequential(\n",
       "    (0): Linear(in_features=23, out_features=200, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): Dropout(p=0.5, inplace=False)\n",
       "    (4): Linear(in_features=200, out_features=100, bias=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (7): Dropout(p=0.5, inplace=False)\n",
       "    (8): Linear(in_features=100, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5b09a287-a593-4328-b55a-aff6394a01b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "24c1a900-eb93-45b4-94eb-47f086441193",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 60000\n",
    "test_size = int(batch_size*0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f823a1bc-6b8e-4c59-aef5-3c3022dccbd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_train = cats[:batch_size-test_size]\n",
    "cat_test = cats[batch_size-test_size:batch_size]\n",
    "cont_train = cont[:batch_size-test_size]\n",
    "cont_test = cont[batch_size-test_size:batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "25a6bc45-146f-415a-93a2-b8568ddaf91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y[:batch_size-test_size]\n",
    "y_test = y[batch_size-test_size:batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "23fece9b-f747-4993-9856-0c57d8f3d2cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 12.589123725891113\n",
      "Epoch: 10, Loss: 11.792752265930176\n",
      "Epoch: 20, Loss: 11.22496223449707\n",
      "Epoch: 30, Loss: 10.832974433898926\n",
      "Epoch: 40, Loss: 10.534828186035156\n",
      "Epoch: 50, Loss: 10.300067901611328\n",
      "Epoch: 60, Loss: 10.087516784667969\n",
      "Epoch: 70, Loss: 9.905826568603516\n",
      "Epoch: 80, Loss: 9.689477920532227\n",
      "Epoch: 90, Loss: 9.486383438110352\n",
      "Epoch: 100, Loss: 9.237733840942383\n",
      "Epoch: 110, Loss: 8.958498001098633\n",
      "Epoch: 120, Loss: 8.639734268188477\n",
      "Epoch: 130, Loss: 8.294915199279785\n",
      "Epoch: 140, Loss: 7.885173797607422\n",
      "Epoch: 150, Loss: 7.464734077453613\n",
      "Epoch: 160, Loss: 7.020777225494385\n",
      "Epoch: 170, Loss: 6.585127353668213\n",
      "Epoch: 180, Loss: 6.103859901428223\n",
      "Epoch: 190, Loss: 5.648401737213135\n",
      "Epoch: 200, Loss: 5.218247413635254\n",
      "Epoch: 210, Loss: 4.8539276123046875\n",
      "Epoch: 220, Loss: 4.600926876068115\n",
      "Epoch: 230, Loss: 4.393165111541748\n",
      "Epoch: 240, Loss: 4.184859752655029\n",
      "Epoch: 250, Loss: 4.173018455505371\n",
      "Epoch: 260, Loss: 4.100866317749023\n",
      "Epoch: 270, Loss: 4.035979747772217\n",
      "Epoch: 280, Loss: 3.968576431274414\n",
      "Epoch: 290, Loss: 3.966066837310791\n",
      "Training Took 0.43057768742243446 mins\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "epochs = 300\n",
    "losses = []\n",
    "\n",
    "for i in range(epochs):\n",
    "\n",
    "    y_pred = model.forward(cat_train, cont_train)\n",
    "    loss = torch.sqrt(criterion(y_pred, y_train))\n",
    "    losses.append(loss.item())\n",
    "\n",
    "    if i%10 == 0:\n",
    "        print(f\"Epoch: {i}, Loss: {loss}\")\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(f\"Training Took {(time.time() - start_time) / 60} mins\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2314865e-5cf8-400f-8b1e-f02e50b36451",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    y_val = model.forward(cat_test, cont_test)\n",
    "    loss = torch.sqrt(criterion(y_val, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3557b1ba-1756-4a43-8448-1faaf47eb014",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.8815)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "879d15eb-1285-452e-a97b-8017625f1e78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   PREDICTED   ACTUAL     DIFF\n",
      " 1.   4.9072   2.9000   2.0072\n",
      " 2.  15.5492   5.7000   9.8492\n",
      " 3.   6.8949   7.7000   0.8051\n",
      " 4.  15.7628  12.5000   3.2628\n",
      " 5.   6.2467   4.1000   2.1467\n",
      " 6.   3.5000   5.3000   1.8000\n",
      " 7.   2.5522   3.7000   1.1478\n",
      " 8.  21.8729  14.5000   7.3729\n",
      " 9.   2.1319   5.7000   3.5681\n",
      "10.  12.3366  10.1000   2.2366\n",
      "11.   7.5209   4.5000   3.0209\n",
      "12.   3.6354   6.1000   2.4646\n",
      "13.   6.3583   6.9000   0.5417\n",
      "14.   9.2390  14.1000   4.8610\n",
      "15.   6.3309   4.5000   1.8309\n",
      "16.  32.4334  34.1000   1.6666\n",
      "17.   1.6804  12.5000  10.8196\n",
      "18.   3.9392   4.1000   0.1608\n",
      "19.   7.9634   8.5000   0.5366\n",
      "20.   3.0906   5.3000   2.2094\n",
      "21.  13.0110  11.3000   1.7110\n",
      "22.  12.1937  10.5000   1.6937\n",
      "23.  15.7102  15.3000   0.4102\n",
      "24.  17.1444  14.9000   2.2444\n",
      "25.  42.2721  49.5700   7.2979\n",
      "26.   3.2670   5.3000   2.0330\n",
      "27.   4.1070   3.7000   0.4070\n",
      "28.   6.3457   6.5000   0.1543\n",
      "29.  14.8165  14.1000   0.7165\n",
      "30.   2.0575   4.9000   2.8425\n",
      "31.   3.6641   3.7000   0.0359\n",
      "32.  26.4817  38.6700  12.1883\n",
      "33.  15.2079  12.5000   2.7079\n",
      "34.  17.3151  16.5000   0.8151\n",
      "35.   5.8215   5.7000   0.1215\n",
      "36.   7.8830   8.9000   1.0170\n",
      "37.  18.5025  22.1000   3.5975\n",
      "38.   9.2370  12.1000   2.8630\n",
      "39.   8.7230  10.1000   1.3770\n",
      "40.   3.1874   3.3000   0.1126\n",
      "41.   9.0253   8.5000   0.5253\n",
      "42.   9.5036   8.1000   1.4036\n",
      "43.   8.7069  14.5000   5.7931\n",
      "44.   7.4524   4.9000   2.5524\n",
      "45.   7.2572   8.5000   1.2428\n",
      "46.  11.3021  12.1000   0.7979\n",
      "47.  26.7551  23.7000   3.0551\n",
      "48.   4.9066   3.7000   1.2066\n",
      "49.   7.9372   9.3000   1.3628\n",
      "50.   8.3639   8.1000   0.2639\n"
     ]
    }
   ],
   "source": [
    "print(f'{\"PREDICTED\":>12} {\"ACTUAL\":>8} {\"DIFF\":>8}')\n",
    "for i in range(50):\n",
    "    diff = np.abs(y_val[i].item()-y_test[i].item())\n",
    "    print(f'{i+1:2}. {y_val[i].item():8.4f} {y_test[i].item():8.4f} {diff:8.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9789179a-a909-4da6-81c1-ea4c414bc177",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"taxifare.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9eb9df-1433-4866-bb1f-3fe704317f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
